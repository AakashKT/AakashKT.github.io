<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://aakashkt.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://aakashkt.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-10T06:58:39+00:00</updated><id>https://aakashkt.github.io/feed.xml</id><title type="html">Aakash KT</title><subtitle>Personal Website. </subtitle><entry><title type="html">Variance analysis - Stratified Sampling v/s Random sampling</title><link href="https://aakashkt.github.io/blog/2023/variance-stratified/" rel="alternate" type="text/html" title="Variance analysis - Stratified Sampling v/s Random sampling"/><published>2023-04-27T00:00:00+00:00</published><updated>2023-04-27T00:00:00+00:00</updated><id>https://aakashkt.github.io/blog/2023/variance-stratified</id><content type="html" xml:base="https://aakashkt.github.io/blog/2023/variance-stratified/"><![CDATA[<p>I was recently reading through the PBRT fourth edition, an exciting followup of the excellent <a href="https://www.pbr-book.org/3ed-2018/contents">third edition</a>. They derive &amp; show that the variance of stratified sampling is necessarily lower than that of random sampling in Chapter 2, Sect 2.2.1 (Page 61). The derivation was not very clear to me and required some thinking/writing of my own. Plus, they refer to Eric Veach’s <a href="http://graphics.stanford.edu/papers/veach_thesis/">thesis</a> for an important expression of the variance (that thesis is a gold mine!). However, I could not for the life of me find a step by step derivation of that expression there too!</p> <p>Well, that’s the motivation for this blog. Writing stuff like this down helps my understanding as well :-).</p> <p><br/></p> <h2 id="definitions">Definitions</h2> <p>We will denote random variables with capital letters like so: \(X\). Expected value is denoted as \(E[\ ]\) &amp; variance is denoted as \(V[\ ]\).</p> <p>Recall the definition of expectation: \(E[f(X)] = \int_{D} f(x) \cdot p(x) dx\), where \(D\) is the domain and \(p(x)\) is the probability of choosing \(x\).</p> <p>Also, recall that variance is expected squared deviation from mean: \(V[f(X)] = E \left [ (f(X) - E[f(X)])^2 \right ]\).</p> <p>Finally, recall that a function of a random variable is still a random variable: \(Y = f(X)\).</p> <p>Let’s first define our target integral over the domain \(\Lambda\):</p> <p> $$ L = \int_{\Lambda} f(x) dx. $$ </p> <p>Lets now define all <b>stratum</b> \(\Lambda_1, \Lambda_2, ... \Lambda_n\), which are non-overlapping and combine to form the whole domain: \(\bigcup_{i=1}^{n} \Lambda_i = \Lambda\). Each stratum has fractional volume \(v_i \in (0, 1]\).</p> <p>The integral within a stratum \(i\) is: \(L_i = \int_{\Lambda_i} f(x) dx\).</p> <p>With this, the target integral can be decomposed like so:</p> <p> $$ L = \int_{\Lambda_1} f(x) dx + \int_{\Lambda_2} f(x) dx + ... + \int_{\Lambda_n} f(x) dx = \sum_i \int_{\Lambda_i} f(x) dx. $$ </p> <p>We will use the following two properties of <a href="https://en.wikipedia.org/wiki/Expected_value#Properties:~:text=Linearity%20of%20expectation%3A%5B34%5D%20The%20expected%20value%20operator%20(or%20expectation%20operator)">expectation</a> and <a href="https://en.wikipedia.org/wiki/Variance#Properties:~:text=If%20all%20values%20are%20scaled%20by%20a%20constant%2C%20the%20variance%20is%20scaled%20by%20the%20square%20of%20that%20constant%3A">variance</a>, given a constant \(a\):</p> <p> $$ E[af(X)] = a E[f(X)], $$ </p> <p> $$ V[af(X)] = a^2 V[f(X)] $$ </p> <p>We will also use the following two properties of <a href="https://en.wikipedia.org/wiki/Expected_value#Properties:~:text=Linearity%20of%20expectation%3A%5B34%5D%20The%20expected%20value%20operator%20(or%20expectation%20operator)">expectation</a> and <a href="https://en.wikipedia.org/wiki/Variance#Properties:~:text=are-,uncorrelated,-%2C%20then%20the%20variance">variance</a>, given random variables \(X_i\) are identically and independently distributed (as is in rendering with Monte Carlo):</p> <p> $$ E \left [ \sum_i f(X_i) \right ] = \sum_i E \left [ f(X_i) \right ], $$ </p> <p> $$ V \left [ \sum_i f(X_i) \right ] = \sum_i V \left [ f(X_i) \right ] $$ </p> <p>Finally, we will make use of the <a href="https://en.wikipedia.org/wiki/Variance#Properties:~:text=2.-,Decomposition,-%5Bedit%5D">decomposition</a> property of variance, for two random varibles \(X, Y\):</p> <p> $$ V[X] = E \left [ V[X | Y] \right ] + V \left [ E[X | Y] \right ], $$ </p> <p>which states that the variance of \(X\) is the sum of mean of the conditional variance and the variance of the conditional mean.</p> <p><br/></p> <h2 id="estimators">Estimators</h2> <p>Using uniform sampling in the stratum \(\Lambda_i\) with volume \(v_i\) using \(n_i\) samples, we can write down it’s monte carlo estimator like so:</p> <p> $$ F_i = \frac{1}{n_i} \sum_{j} \frac{f(X_{i, j})}{1 / v_i} = \frac{v_i}{n_i} \sum_{j} f(X_{i, j}), $$ </p> <p>where \(X_{i, j}\) is the jth sample in the ith stratum.</p> <p>The estimator of the target integral can be written in terms of estimators of individual strata like so:</p> <p> $$ F = \sum_i F_i. $$ </p> <p><br/></p> <h2 id="variance-of-stratified-sampling">Variance of Stratified Sampling</h2> <p>Denote the variance within a stratum as: \(\sigma_i^2 = V[f(X_{i, j})]\).</p> <p>Now, the vaiance of the stratum estimator \(F_i\) is:</p> <p> $$ V[F_i] = V \left [ \frac{v_i}{n_i} \sum_j f(X_{i, j}) \right ] $$ </p> <p>Using the first set of properties:</p> <p> $$ V[F_i] = \frac{v_i^2}{n_i^2} V \left [ \sum_j f(X_{i, j}) \right ] $$ </p> <p>Using the second set of properties:</p> <p> $$ V[F_i] = \frac{v_i^2}{n_i} \cdot \frac{1}{n_i} \sum_j V \left [ f(X_{i, j}) \right ] $$ $$ V[F_i] = \frac{v_i^2}{n_i} \cdot \frac{1}{n_i} \sum_j \sigma_i^2 $$ $$ V[F_i] = \frac{v_i^2}{n_i} \cdot \sigma_i^2 \cdot \frac{1}{n_i} \sum_j 1 $$ $$ V[F_i] = \frac{v_i^2 \sigma_i^2}{n_i} $$ </p> <p>Finally, the variance of the target estimator is:</p> <p> $$ V[F] = V[\sum_i F_i] = \sum_i V[F_i] $$ $$ V[F] = \sum_i \frac{v_i^2 \sigma_i^2}{n_i} $$ </p> <p>Typically, we take \(n_i = v_i n\), thus:</p> <p> $$ \boxed{ V[F] = \frac{1}{n} \sum_i v_i \sigma_i^2 } $$ </p> <p><br/></p> <h2 id="variance-of-random-sampling">Variance of Random Sampling</h2> <p>Random sampling can be done within this framework by first choosing a stratum \(i\) according to discrete probabilities defined by \(v_i\) and then choosing a sample uniformly within that stratum.</p> <p>Define probabilities of choosing strata like so (since \(v_i \in (0, 1]\)):</p> <p> $$ p(i) = v_i, $$ </p> <p>and the probability of uniformly choosing a sample \(j\) given a stratum as:</p> <p> $$ p(X_{i, j} | i) = \frac{1}{v_i}. $$ </p> <p>With this, the target estimator is:</p> <p> $$ F = \frac{1}{n} \sum_j \frac{f(X_{i, j})}{p(X_{i, j} | i) p(i)} $$ $$ F = \frac{1}{n} \sum_j f(X_{i, j}) $$ </p> <hr/> <p>We are now choosing \(X_{i, j}\) <b>conditionally</b> on \(I\) (random variable on strata).</p> <p>Variance of \(f(X_{i, j})\) is not just witin a stratum like defined before. But, we can use the <b>decomposition</b> property to get variance of \(f(X_{i, j})\):</p> <p> $$ V[f(X_{i, j})] = E \left [ V[f(X_{i, j}) | I] \right ] + V \left [ E[f(X_{i, j}) | I] \right ] $$ </p> <hr/> <p>Lets look a the first term in the sum: \(E[V[..]]\). The variance is of \(f(X_{i, j})\) given a stratum, which is exactly \(\sigma_i^2\). Thus,</p> <p> $$ E \left [ V[f(X_{i, j}) | I] \right ] = E[\sigma_i^2] $$ </p> <p>Since expectation over strata are on discrete probabilities:</p> <p> $$ E \left [ V[f(X_{i, j}) | I] \right ] = \sum_i \sigma_i^2 \cdot p(i) $$ $$ E \left [ V[f(X_{i, j}) | I] \right ] = \sum_i \sigma_i^2 v_i $$ </p> <hr/> <p>Lets now look at the second term: \(V[E[..]]\). Here, the expected value is of \(f(X_{i, j})\) given a stratum, which is exactly \(L_i\).</p> <p> $$ V \left [ E[f(X_{i, j}) | I] \right ] = V[L_i] $$ </p> <p>In this case, the variance will be with respect to the target integral.</p> <p> $$ V \left [ E[f(X_{i, j}) | I] \right ] = E[(L_i - L)^2] $$ </p> <p>Again, due to discrete probabilities:</p> <p> $$ V \left [ E[f(X_{i, j}) | I] \right ] = \sum_i (L_i - L)^2 v_i $$ </p> <hr/> <p>We now get an expression of the variance of \(f(X_{i, j})\) for random sampling:</p> <p> $$ V[f(X_{i, j})] = \sum_i \sigma_i^2 v_i + \sum_i (L_i - L)^2 v_i $$ </p> <hr/> <p>Finally, the variance of the target estimator is:</p> <p> $$ V[F] = V \left [ \frac{1}{n} \sum_j f(X_{i, j}) \right ] $$ </p> <p>Again, using the first and second set of properties:</p> <p> $$ V[F] = \frac{1}{n^2} \sum_j V \left [ f(X_{i, j}) \right ] = \frac{1}{n} \cdot \frac{1}{n} \sum_j V \left [ f(X_{i, j}) \right ] $$ $$ V[F] = \frac{1}{n} V \left [ f(X_{i, j}) \right ] $$ $$ \boxed{ V[F] = \frac{1}{n} \left [ \sum_i \sigma_i^2 v_i + \sum_i (L_i - L)^2 v_i \right ] } $$ </p> <p><br/></p> <h2 id="conclusion">Conclusion</h2> <p>The first and the second boxed equations show the variance of stratified and random sampling respectively. Its clear that the latter’s variance is larger due to the additional term.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Prove that variance of stratified sampling is lower as compared to random sampling.]]></summary></entry><entry><title type="html">Jacobian of the LTC transformation</title><link href="https://aakashkt.github.io/blog/2022/ltc-jacobian/" rel="alternate" type="text/html" title="Jacobian of the LTC transformation"/><published>2022-06-11T00:00:00+00:00</published><updated>2022-06-11T00:00:00+00:00</updated><id>https://aakashkt.github.io/blog/2022/ltc-jacobian</id><content type="html" xml:base="https://aakashkt.github.io/blog/2022/ltc-jacobian/"><![CDATA[<p>My recent <a href="ltc_anisotropic.html">paper</a> builds on this <a href="https://eheitzresearch.wordpress.com/415-2/">Linearly Transformed Cosines paper</a> by Eric Heitz. Infact, this was a collaboration with Eric himself! Linearly Tranformed Cosines or LTCs are a transformation of the rendering integral under specific assumptions, which lead to analytic solutions of it. One of the outcomes of my new work was the relaxation of the isotropic assumption of LTCs, allowing the use of anisotropic BRDFs.</p> <p>Although I have read the original paper countless times and worked on an extension of it, I still was puzzled by the LTC jacobian derivation given in the appendix of the original paper:</p> <p><br/></p> <div class="row" align="center"> <div class="col-sm-6"> <img src="/assets/img/ltc_jac_1.png"/> </div> <div class="col-sm-6"> <img src="/assets/img/ltc_jac_2.png"/> </div> </div> <div class="row" align="center"> <div class="col-sm-12"> <b>Figure 1:</b> LTC jacobian derivation from the <a href="https://eheitzresearch.wordpress.com/415-2/">original paper.</a> </div> </div> <p><br/></p> <p>The derivation must be pretty obvious to a lot of people but to me it wasn’t so. This post is an attempt to give a detailed and intuitive derivation of this expression (Eq. 18 above). Why is this important? Well, in my view small details like these expand your horizon, and you can only hope to get new ideas if you throughly understand what’s already out there. Also, its always fun to understand things from a mathematical perspective, even if you are a intuitive-first person like I am.</p> <p>I would like to note that figures in this post have been directly taken from the <a href="https://eheitzresearch.wordpress.com/415-2/">original paper.</a></p> <p><br/></p> <h2 id="definitions">Definitions</h2> <p>Its best if you have read and are confortable with Eric’s original paper. However, I am still defining things here to ensure consistency.</p> <p>Direction vectors on the unit sphere are given by \(\omega = (x, y, z)\). \(D_o(\omega_o)\) is the source (clamped cosine) distribution. \(D(\omega)\) is the target (~ GGX BRDF) distribution. Given an LTC matrix \(M\), the direction vectors \(\omega\) of \(D\) are obtained by from the direction vectors \(\omega_o\) of \(D_o\) by:</p> <p> $$ \omega = \frac{M\omega_o}{|| M\omega_o ||}. $$ </p> <p>The magnitude at the transformed direction is given by:</p> <p> $$ \boxed{ D(\omega) = D_o(\omega_o) \frac{\partial \omega_o}{\partial \omega}. } $$ </p> <p>Here, \(\frac{\partial \omega_o}{\partial \omega}\) is the determinant of jacobian of the transformation (which is frequently referred to as just the jacobian). This is what we will derive in this post.</p> <p>Another thing we will find useful is the <a href="https://en.wikipedia.org/wiki/Solid_angle#:~:text=Thus%20one%20can,the%20viewer%20as%3A">solid angle subtended by a small facet</a> having area \(A\), which is given by:</p> <p> $$ \partial \omega = \frac{A (\hat{r} \cdot \hat{n})}{r^2}, $$ </p> <p>where \(\partial\omega\) is a small solid angle, \(\hat{r}\) is the normalized position vector of the facet, \(\hat{n}\) is the surface normal of the facet and \(r^2\) is the distance to the facet.</p> <p>Finally, we will find use of the <a href="https://en.wikipedia.org/wiki/Cross_product#:~:text=More%20generally%2C%20the%20cross%20product%20obeys%20the%20following%20identity%20under%20matrix%20transformations%3A"> cross product under linear transforms</a> property:</p> <p> $$ M\omega_1 \times M\omega_2 = |M| M^{-T} (\omega_1 \times \omega_2) = |M| M^{-T} \omega_o $$ </p> <p><br/></p> <h2 id="derivation">Derivation</h2> <p>First things first. Note that \(D\) and \(D_o\) are distributions in <b>2D</b>, although they appear to be distributions in 3D. To see why, note that these are distributions on the unit sphere. Hence, we can write \(\omega = (\sin\theta \cos\phi, \sin\theta \sin\phi, \cos\theta)\), in turn making \(D\) and \(D_o\) dependent on only \(\theta, \phi\) (thus two dimensional).</p> <p>From the <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant#:~:text=Jacobian%20determinant%5Bedit%5D">definition of jacobian determinants</a>, \(\frac{\partial \omega_o}{\partial \omega}\) measures how a differential area around \(\omega_o\) changes. Since this differential area in on the unit sphere, it is basically the <a href="https://en.wikipedia.org/wiki/Solid_angle">solid angle</a>. <b>The LTC jacobian thus measures change in solid angle under the LTC transform</b>.</p> <p>Our problem now boils down to determining how a differential solid angle changes under \(M\).</p> <p>Let’s take the differential solid angle \(\color{red}\partial\omega_o\) in \(D_o\). Since this is small enough, it can also be considered as a planar facet. Now, define two vectors \(\omega_1, \omega_2\), which together with \(\omega_o\) form an orthonormal basis. (Figure 2, left)</p> <div class="row" align="center"> <div class="col-sm-12"> <img src="/assets/img/do_solid_angle.png" style="width: 600px"/> </div> </div> <div class="row" align="center"> <div class="col-sm-12"> <b>Figure 2:</b> Left: Orthonormal basis in D<sub>o</sub>; Right: The same basis transformed by M. </div> </div> <p><br/></p> <p>Transforming these vectors by LTC \(M\) results in a new set of vectors (Figure 2, right). The area of the facet \(\color{red}\partial\omega_o\) is now changed to \(\color{red}\partial\omega_o \cdot A\), where \(A\) is the area of parallelogram defined by vectors \(M\omega_1\) and \(M\omega_2\).</p> <p>A subtelty you may want to note, is that in Figure 2 left, the area of the parallelogram given by \(\omega_1\) and \(\omega_2\) is \(1\), thus the differential area is only \(\partial\omega_o\). This changes in Figure 2 right, since the area of the parallelogram is not \(1\) anymore, and thus the differential area is \(\partial\omega_o \cdot A\).</p> <p>Alright! Now how about the differential solid angle \(\color{green}\partial\omega\) after the LTC transform? Its essentialy the <b>solid angle subtended by a small facet</b>! We know this directly (see definitions section):</p> <p> $$ \partial\omega = \frac{\partial\omega_o A (\hat{r} \cdot \hat{n})}{r^2}. $$ </p> <p>The terms \(\hat{r}, \hat{n}\) and \(A\) are simple to define in this case.</p> <p>The area is \(A = \|M\omega_1 \times M\omega_2\|\), which is a <a href="https://en.wikipedia.org/wiki/Cross_product">standard formula.</a></p> <p>The unit position vector is \(\hat{r} = \frac{M\omega_o}{\|M\omega_o\|}\) (the division just normalizes since \(\hat{r}\) is a unit vector).</p> <p>And finally, the normal vector is \(\hat{n} = \frac{M\omega_1 \times M\omega_2}{\|M\omega_1 \times M\omega_2\|}\) (again, division for normalization).</p> <p>Lets first simplify the dot product \(\hat{r} \cdot \hat{n}\) by plugging in the above definitions:</p> <p> $$ \hat{r} \cdot \hat{n} = \left( \frac{M\omega_o}{\|M\omega_o\|} \right)^T \frac{M\omega_1 \times M\omega_2}{\|M\omega_1 \times M\omega_2\|} $$ </p> <p>Using <b>coss product under linear transforms</b> property:</p> <p> $$ \hat{r} \cdot \hat{n} = \frac{(M\omega_o)^T}{\|M\omega_o\|} \frac{|M| M^{-T} \omega_o}{\|M\omega_1 \times M\omega_2\|} $$ $$ \ \ \ \ \ \ = \frac{|M| (M\omega_o)^T M^{-T} \omega_o}{\|M\omega_o\| \|M\omega_1 \times M\omega_2\|} $$ </p> <p>Using the fact that \((M\omega_o)^T = \omega_o^T M^T\):</p> <p> $$ \ \ \ \ \ \ = \frac{|M| \omega_o^{T}{\color{red}M^T M^{-T}} \omega_o}{\|M\omega_o\| \|M\omega_1 \times M\omega_2\|} $$ $$ \ \ \ \ \ \ = \frac{|M| {\color{red}\omega_o^{T} \omega_o}}{\|M\omega_o\| \|M\omega_1 \times M\omega_2\|} $$ </p> <p>\(\omega_o^T\omega_o\) is equal to \(1\), since \(\omega_o\) is a unit vector.</p> <p> $$ \Longrightarrow \hat{r} \cdot \hat{n} = \frac{|M|}{\|M\omega_o\| \|M\omega_1 \times M\omega_2\|} $$ </p> <p>Finally, substitue into and simplify the expression for \(\partial\omega\):</p> <p> $$ \partial\omega = \frac{\partial\omega_o A (\hat{r} \cdot \hat{n})}{r^2} $$ $$ \frac{\partial\omega}{\partial\omega_o} = \frac{A (\hat{r} \cdot \hat{n})}{r^2} $$ $$ \Longrightarrow \frac{\partial\omega}{\partial\omega_o} = \frac{ {\color{red}\|M\omega_1 \times M\omega_2\|} }{\|M\omega_o\|^2} \frac{|M|}{\|M\omega_o\| {\color{red} \|M\omega_1 \times M\omega_2\|} } $$ $$ \boxed{ \Longrightarrow \frac{\partial\omega}{\partial\omega_o} = \frac{|M|}{\|M\omega_o\|^3}. } $$ </p> <p>This is what we are looking for! :smile:</p> <p><br/></p> <h2 id="conclusion">Conclusion</h2> <p>This is a pretty intuitive proof, given that you think and go about it the right way. Hope this post helps some of you in your own research journey!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Deriving the jacobian of the LTC transform. In-detail walkthrough.]]></summary></entry></feed>