<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Variance analysis - Stratified Sampling v/s Random sampling | Aakash KT</title> <meta name="author" content="Aakash KT"/> <meta name="description" content="Prove that variance of stratified sampling is lower as compared to random sampling."/> <meta name="keywords" content="aakashkt, real-time-rendering, computer-graphics, ray-tracing, path-tracing"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://aakashkt.github.io/blog/2023/variance-stratified/"> <link rel="stylesheet" href="assets/css/image_comparison_style.css"> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script> <script src="assets/js/image_comparison.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://aakashkt.github.io//">Aakash KT</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Variance analysis - Stratified Sampling v/s Random sampling</h1> <p class="post-meta">April 27, 2023</p> <hr> </header> <article class="post-content"> <p>I was recently reading through the PBRT fourth edition, an exciting followup of the excellent <a href="https://www.pbr-book.org/3ed-2018/contents" target="_blank" rel="noopener noreferrer">third edition</a>. They derive &amp; show that the variance of stratified sampling is necessarily lower than that of random sampling in Chapter 2, Sect 2.2.1 (Page 61). The derivation was not very clear to me and required some thinking/writing of my own. Plus, they refer to Eric Veach’s <a href="http://graphics.stanford.edu/papers/veach_thesis/" target="_blank" rel="noopener noreferrer">thesis</a> for an important expression of the variance (that thesis is a gold mine!). However, I could not for the life of me find a step by step derivation of that expression there too!</p> <p>Well, that’s the motivation for this blog. Writing stuff like this down helps my understanding as well :-).</p> <p><br></p> <h2 id="definitions">Definitions</h2> <p>We will denote random variables with capital letters like so: \(X\). Expected value is denoted as \(E[\ ]\) &amp; variance is denoted as \(V[\ ]\).</p> <p>Recall the definition of expectation: \(E[f(X)] = \int_{D} f(x) \cdot p(x) dx\), where \(D\) is the domain and \(p(x)\) is the probability of choosing \(x\).</p> <p>Also, recall that variance is expected squared deviation from mean: \(V[f(X)] = E \left [ (f(X) - E[f(X)])^2 \right ]\).</p> <p>Finally, recall that a function of a random variable is still a random variable: \(Y = f(X)\).</p> <p>Let’s first define our target integral over the domain \(\Lambda\):</p> <p> $$ L = \int_{\Lambda} f(x) dx. $$ </p> <p>Lets now define all <b>stratum</b> \(\Lambda_1, \Lambda_2, ... \Lambda_n\), which are non-overlapping and combine to form the whole domain: \(\bigcup_{i=1}^{n} \Lambda_i = \Lambda\). Each stratum has fractional volume \(v_i \in (0, 1]\).</p> <p>The integral within a stratum \(i\) is: \(L_i = \int_{\Lambda_i} f(x) dx\).</p> <p>With this, the target integral can be decomposed like so:</p> <p> $$ L = \int_{\Lambda_1} f(x) dx + \int_{\Lambda_2} f(x) dx + ... + \int_{\Lambda_n} f(x) dx = \sum_i \int_{\Lambda_i} f(x) dx. $$ </p> <p>We will use the following two properties of <a href="https://en.wikipedia.org/wiki/Expected_value#Properties:~:text=Linearity%20of%20expectation%3A%5B34%5D%20The%20expected%20value%20operator%20(or%20expectation%20operator)" target="_blank" rel="noopener noreferrer">expectation</a> and <a href="https://en.wikipedia.org/wiki/Variance#Properties:~:text=If%20all%20values%20are%20scaled%20by%20a%20constant%2C%20the%20variance%20is%20scaled%20by%20the%20square%20of%20that%20constant%3A" target="_blank" rel="noopener noreferrer">variance</a>, given a constant \(a\):</p> <p> $$ E[af(X)] = a E[f(X)], $$ </p> <p> $$ V[af(X)] = a^2 V[f(X)] $$ </p> <p>We will also use the following two properties of <a href="https://en.wikipedia.org/wiki/Expected_value#Properties:~:text=Linearity%20of%20expectation%3A%5B34%5D%20The%20expected%20value%20operator%20(or%20expectation%20operator)" target="_blank" rel="noopener noreferrer">expectation</a> and <a href="https://en.wikipedia.org/wiki/Variance#Properties:~:text=are-,uncorrelated,-%2C%20then%20the%20variance" target="_blank" rel="noopener noreferrer">variance</a>, given random variables \(X_i\) are identically and independently distributed (as is in rendering with Monte Carlo):</p> <p> $$ E \left [ \sum_i f(X_i) \right ] = \sum_i E \left [ f(X_i) \right ], $$ </p> <p> $$ V \left [ \sum_i f(X_i) \right ] = \sum_i V \left [ f(X_i) \right ] $$ </p> <p>Finally, we will make use of the <a href="https://en.wikipedia.org/wiki/Variance#Properties:~:text=2.-,Decomposition,-%5Bedit%5D" target="_blank" rel="noopener noreferrer">decomposition</a> property of variance, for two random varibles \(X, Y\):</p> <p> $$ V[X] = E \left [ V[X | Y] \right ] + V \left [ E[X | Y] \right ], $$ </p> <p>which states that the variance of \(X\) is the sum of mean of the conditional variance and the variance of the conditional mean.</p> <p><br></p> <h2 id="estimators">Estimators</h2> <p>Using uniform sampling in the stratum \(\Lambda_i\) with volume \(v_i\) using \(n_i\) samples, we can write down it’s monte carlo estimator like so:</p> <p> $$ F_i = \frac{1}{n_i} \sum_{j} \frac{f(X_{i, j})}{1 / v_i} = \frac{v_i}{n_i} \sum_{j} f(X_{i, j}), $$ </p> <p>where \(X_{i, j}\) is the jth sample in the ith stratum.</p> <p>The estimator of the target integral can be written in terms of estimators of individual strata like so:</p> <p> $$ F = \sum_i F_i. $$ </p> <p><br></p> <h2 id="variance-of-stratified-sampling">Variance of Stratified Sampling</h2> <p>Denote the variance within a stratum as: \(\sigma_i^2 = V[f(X_{i, j})]\).</p> <p>Now, the vaiance of the stratum estimator \(F_i\) is:</p> <p> $$ V[F_i] = V \left [ \frac{v_i}{n_i} \sum_j f(X_{i, j}) \right ] $$ </p> <p>Using the first set of properties:</p> <p> $$ V[F_i] = \frac{v_i^2}{n_i^2} V \left [ \sum_j f(X_{i, j}) \right ] $$ </p> <p>Using the second set of properties:</p> <p> $$ V[F_i] = \frac{v_i^2}{n_i} \cdot \frac{1}{n_i} \sum_j V \left [ f(X_{i, j}) \right ] $$ $$ V[F_i] = \frac{v_i^2}{n_i} \cdot \frac{1}{n_i} \sum_j \sigma_i^2 $$ $$ V[F_i] = \frac{v_i^2}{n_i} \cdot \sigma_i^2 \cdot \frac{1}{n_i} \sum_j 1 $$ $$ V[F_i] = \frac{v_i^2 \sigma_i^2}{n_i} $$ </p> <p>Finally, the variance of the target estimator is:</p> <p> $$ V[F] = V[\sum_i F_i] = \sum_i V[F_i] $$ $$ V[F] = \sum_i \frac{v_i^2 \sigma_i^2}{n_i} $$ </p> <p>Typically, we take \(n_i = v_i n\), thus:</p> <p> $$ \boxed{ V[F] = \frac{1}{n} \sum_i v_i \sigma_i^2 } $$ </p> <p><br></p> <h2 id="variance-of-random-sampling">Variance of Random Sampling</h2> <p>Random sampling can be done within this framework by first choosing a stratum \(i\) according to discrete probabilities defined by \(v_i\) and then choosing a sample uniformly within that stratum.</p> <p>Define probabilities of choosing strata like so (since \(v_i \in (0, 1]\)):</p> <p> $$ p(i) = v_i, $$ </p> <p>and the probability of uniformly choosing a sample \(j\) given a stratum as:</p> <p> $$ p(X_{i, j} | i) = \frac{1}{v_i}. $$ </p> <p>With this, the target estimator is:</p> <p> $$ F = \frac{1}{n} \sum_j \frac{f(X_{i, j})}{p(X_{i, j} | i) p(i)} $$ $$ F = \frac{1}{n} \sum_j f(X_{i, j}) $$ </p> <hr> <p>We are now choosing \(X_{i, j}\) <b>conditionally</b> on \(I\) (random variable on strata).</p> <p>Variance of \(f(X_{i, j})\) is not just witin a stratum like defined before. But, we can use the <b>decomposition</b> property to get variance of \(f(X_{i, j})\):</p> <p> $$ V[f(X_{i, j})] = E \left [ V[f(X_{i, j}) | I] \right ] + V \left [ E[f(X_{i, j}) | I] \right ] $$ </p> <hr> <p>Lets look a the first term in the sum: \(E[V[..]]\). The variance is of \(f(X_{i, j})\) given a stratum, which is exactly \(\sigma_i^2\). Thus,</p> <p> $$ E \left [ V[f(X_{i, j}) | I] \right ] = E[\sigma_i^2] $$ </p> <p>Since expectation over strata are on discrete probabilities:</p> <p> $$ E \left [ V[f(X_{i, j}) | I] \right ] = \sum_i \sigma_i^2 \cdot p(i) $$ $$ E \left [ V[f(X_{i, j}) | I] \right ] = \sum_i \sigma_i^2 v_i $$ </p> <hr> <p>Lets now look at the second term: \(V[E[..]]\). Here, the expected value is of \(f(X_{i, j})\) given a stratum, which is exactly \(L_i\).</p> <p> $$ V \left [ E[f(X_{i, j}) | I] \right ] = V[L_i] $$ </p> <p>In this case, the variance will be with respect to the target integral.</p> <p> $$ V \left [ E[f(X_{i, j}) | I] \right ] = E[(L_i - L)^2] $$ </p> <p>Again, due to discrete probabilities:</p> <p> $$ V \left [ E[f(X_{i, j}) | I] \right ] = \sum_i (L_i - L)^2 v_i $$ </p> <hr> <p>We now get an expression of the variance of \(f(X_{i, j})\) for random sampling:</p> <p> $$ V[f(X_{i, j})] = \sum_i \sigma_i^2 v_i + \sum_i (L_i - L)^2 v_i $$ </p> <hr> <p>Finally, the variance of the target estimator is:</p> <p> $$ V[F] = V \left [ \frac{1}{n} \sum_j f(X_{i, j}) \right ] $$ </p> <p>Again, using the first and second set of properties:</p> <p> $$ V[F] = \frac{1}{n^2} \sum_j V \left [ f(X_{i, j}) \right ] = \frac{1}{n} \cdot \frac{1}{n} \sum_j V \left [ f(X_{i, j}) \right ] $$ $$ V[F] = \frac{1}{n} V \left [ f(X_{i, j}) \right ] $$ $$ \boxed{ V[F] = \frac{1}{n} \left [ \sum_i \sigma_i^2 v_i + \sum_i (L_i - L)^2 v_i \right ] } $$ </p> <p><br></p> <h2 id="conclusion">Conclusion</h2> <p>The first and the second boxed equations show the variance of stratified and random sampling respectively. Its clear that the latter’s variance is larger due to the additional term.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Aakash KT. Aakash KT Last updated: August 10, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>